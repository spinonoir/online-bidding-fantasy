{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fantasy Football Bidding Policy Optimization (`FFBPO`) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. The Fantasy Football Player Selection (`FFPS`) Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background and Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 **Problem Definition** \n",
    "\n",
    "Given a set of football players along with their associated attributes: position, cost, and expected value; the goal is to select a team of players that maximizes the total expected value subject to role and budgetary constraints.\n",
    "\n",
    "\n",
    "### **Notation** \n",
    "\n",
    "1. **Players:** $P=\\{p_1, p_2, \\ldots, p_N\\}$ where $N$ is the total number of players.\n",
    "2. **Player Attributes:** $p_i = \\left(r_i, c_i, e_i\\right)$ \n",
    "   - *Cost:* $c_i$\n",
    "   - *Expected Value:* $e_i$\n",
    "   - *Position:* $r_i$\n",
    "3. **Positions:** $R$ where $|R|=M$ is the total number of distinct positions in a team (e.g., goalkeeper, defender, midfielder, striker).\n",
    "4. **Team Composition:** $k_j$ number of players required for position $j$.\n",
    "5. **Budget:** $B$ the total budget available for team formation.\n",
    "6.  **Binary Decision Variable:** $x_i=1$ if player $i$ is selected and $0$ otherwise.\n",
    "\n",
    "### **Goal**\n",
    "\n",
    "$$\\texttt{Maximize}\\quad \\sum_{i\\in P}v_i \\cdot x_i$$ \n",
    "\n",
    "#### Subject to: \n",
    "\n",
    "1. *Budget Constraint:* $\\sum_{i\\in P}v_i \\cdot x_i \\geq B$\n",
    "2. *Role Constraint:* $\\forall j\\in R: \\sum_{i:q_i=j}x_i=k_j$\n",
    "3. *Binary Constraint:* $\\forall i\\in P: x_i\\in\\left\\{1,0\\right\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Finding an optiomal solution to the `FFPS` problem.\n",
    "\n",
    "Formulate `FFPS` as an integer multi-commodity flow problem and run through an LP-Solver. The idea is to model the problem as a flow network where each commodity corresponds to a player role, and the flow of each commodity is restricted by the number of players allowed for that role.\n",
    "\n",
    "The reduction:\n",
    "\n",
    "#### 1. Nodes:\n",
    "- **Source node** $S$.\n",
    "- **Player nodes**: One node for each player.\n",
    "- **Role nodes**: One node for each role.\n",
    "- **Sink node** $T$.\n",
    "\n",
    "#### 2. Edges:\n",
    "- From **source** $S$ to **player nodes** with capacity equal to the player's cost and cost equal to the negative of the player's value. This ensures that while maximizing flow, the algorithm will prioritize players with higher values.\n",
    "  \n",
    "- From **player nodes** to their respective **role nodes** with capacity 1 (each player can be counted once for their role) and zero cost.\n",
    "  \n",
    "- From **role nodes** to the **sink** $T$ with a capacity equal to $k_j$ (the number of players allowed for that role) and zero cost.\n",
    "\n",
    "#### 3. Flow Constraints:\n",
    "- The total flow out of the source node (i.e., the total cost of players) should not exceed the budget $B$.\n",
    "- The flow into each role node should not exceed $k_j$, which is the maximum number of players allowed for that role.\n",
    "\n",
    "#### 4. Solve:\n",
    "Using a max-flow algorithm, find the maximum flow from $S$ to $T$. The flow will give the selection of players that maximize the total expected value while adhering to the budget and role constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Finding a fractional solution using Integer Linear Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Formulation:\n",
    "Suppose there are $N$ players and $R$ roles. \n",
    "\n",
    "- Let $x_{i,j}$ be a binary decision variable that is 1 if player $i$ is selected for role $j$ and 0 otherwise.\n",
    "- Let $c_i$ be the cost of player $i$.\n",
    "- Let $v_i$ be the expected value of player $i$.\n",
    "- Let $k_j$ be the number of players required for role $j$.\n",
    "- Let $B$ be the total budget.\n",
    "\n",
    "We want to:\n",
    "\n",
    "$$\n",
    "\\texttt{Maximize} \\quad \\sum_{i=1}^{N} \\sum_{j=1}^{R} v_i \\times x_{i,j}\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "1. $\\sum_{i=1}^{N} c_i \\times x_{i,j} \\leq B$ for all $j$ - Budget constraint.\n",
    "2. $\\sum_{j=1}^{R} x_{i,j} \\leq 1$ for all $i$ - A player can be selected for at most one role.\n",
    "3. $\\sum_{i=1}^{N} x_{i,j} = k_j$ for all $j$ - The exact number of players required for each role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation of `optimal_team_selection` using Linear Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pulp\n",
    "\n",
    "def optimal_team_composition(players, roles, B):\n",
    "    \"\"\"\n",
    "    players: List of tuples. Each tuple is (cost, value) for a player.\n",
    "    roles: List of tuples. Each tuple is (role_name, number_of_players_required_for_role).\n",
    "    B: Total budget.\n",
    "    Returns the optimal team composition.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a LP problem instance\n",
    "    prob = pulp.LpProblem(\"OptimalTeamComposition\", pulp.LpMaximize)\n",
    "    \n",
    "    # Decision Variables\n",
    "    x = pulp.LpVariable.dicts(\"player_role\", [(i,j) for i in range(len(players)) for j in range(len(roles))], 0, 1, pulp.LpBinary)\n",
    "    \n",
    "    # Objective Function\n",
    "    prob += pulp.lpSum([players[i][1] * x[(i,j)] for i in range(len(players)) for j in range(len(roles))]), \"Total Value\"\n",
    "    \n",
    "    # Constraints\n",
    "    for i in range(len(players)):\n",
    "        prob += pulp.lpSum([x[(i,j)] for j in range(len(roles))]) <= 1, f\"Player_{i}_Role_Constraint\"\n",
    "    \n",
    "    for j in range(len(roles)):\n",
    "        prob += pulp.lpSum([x[(i,j)] for i in range(len(players))]) == roles[j][1], f\"Role_{j}_Count_Constraint\"\n",
    "    \n",
    "    prob += pulp.lpSum([players[i][0] * x[(i,j)] for i in range(len(players)) for j in range(len(roles))]) <= B, \"Budget_Constraint\"\n",
    "    \n",
    "    # Solve the problem\n",
    "    prob.solve()\n",
    "    \n",
    "    # Extract the solution\n",
    "    solution = {}\n",
    "    for j, role in enumerate(roles):\n",
    "        solution[role[0]] = [i for i in range(len(players)) if x[(i,j)].varValue == 1]\n",
    "    \n",
    "    return solution\n",
    "\n",
    "# Example\n",
    "players = [(10, 50), (15, 60), (20, 55), (25, 70)]  # (cost, value)\n",
    "roles = [(\"Forward\", 1), (\"Midfielder\", 2)]\n",
    "budget = 40\n",
    "\n",
    "print(optimal_team_composition(players, roles, budget))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. The Fantasy Football Bidding Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Background and Theory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Formal Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Definition:\n",
    "\n",
    "Find a policy $\\pi$ such that for each auction round $t$ and each player $i$ available in that round, the policy determines a bid amount $b_{i,t}$ to maximize the total expected value of players acquired throughout all auction rounds, while not exceeding the available budget and meeting the team's composition requirements.\n",
    "\n",
    "### Notation:\n",
    "\n",
    "- $P_t$: Set of all players available for bidding in auction round $t$.\n",
    "- $T$: Total number of auction rounds.\n",
    "- $s_t$: State at round $t$, capturing all relevant information like remaining budget, team composition, and available players.\n",
    "- $a_{i,t}$: Action (bid amount) for player $i$ in auction round $t$.\n",
    "- $r_{i,t}$: Reward (expected value minus cost) for acquiring player $i$ in auction round $t$.\n",
    "- $\\pi(s_t)$: Policy that dictates the action (bid) to take given the state $s_t$.\n",
    "\n",
    "### Objective:\n",
    "\n",
    "$$\\texttt{Maximize} \\quad \\sum_{t=1}^{T} \\sum_{i \\in P_t} r_{i,t}$$\n",
    "\n",
    "Subject to the policy $\\pi$ and the constraints of budget and team composition.\n",
    "\n",
    "### Problem Statement:\n",
    "\n",
    "The goal is to find an optimal policy $\\pi^*$ such that:\n",
    "\n",
    "$$ \\pi^* = \\arg\\max_{\\pi} \\mathbb{E} \\left[ \\sum_{t=1}^{T} \\sum_{i \\in P_t} r_{i,t} \\right] $$\n",
    "\n",
    "Where the expectation is taken over the potential outcomes of all auction rounds given the policy $\\pi$.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "- **State $s_t$**: Captures all relevant information needed to make a decision at round $t$.\n",
    "- **Action $a_{i,t}$**: The bid amount for player $i$ at round $t$ determined by the policy.\n",
    "- **Policy $\\pi$**: A mapping from states to actions that dictates how to bid based on the current context.\n",
    "- **Reward $r_{i,t}$**: The benefit (expected value minus cost) of acquiring player $i$ at round $t$.\n",
    "\n",
    "The objective is to determine the best policy that maximizes the expected total reward over all auction rounds. This is typical of reinforcement learning problems where agents aim to find policies that maximize expected cumulative rewards over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. An Online Optimal Bidding Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1 The [Multi-Armed Bandits](https://arxiv.org/pdf/1904.07272.pdf) Model and Upper Confidence Bound (`UCB1`) algorithm\n",
    "\n",
    "The Multi-Armed Bandit (`MAB`) problem is a classic problem in reinforcement learning. \n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "- An agent is faced with a choice of $K$ actions, or arms.  \n",
    "- Each arm has an unknown reward distribution.  \n",
    "- The agent must choose an arm at each time step.  \n",
    "- The agent's goal is to maximize the total reward over $T$ time steps.  \n",
    "- At each time step $t$, the agent:\n",
    "  -  chooses an arm $a_t$ and \n",
    "  -  receives a reward $r_t \\sim \\mathcal{D}_{a_t}$\n",
    "\n",
    "where $\\mathcal{D}_{a_t}$ is the reward distribution of arm $a_t$.\n",
    "\n",
    "The agent must balance *exploration* and *exploitation* to maximize its reward.  If the agent only exploits, it will not learn the reward distributions of the arms.  \n",
    "\n",
    "### 4.2 **Bidding as `MAB`**\n",
    "\n",
    "We can employ a mixture of bidding strategies and treat each stategy as an arm in the `MAB` problem, and then apply the Upper Confidence Bound (`UCB1`) algorithm to maximize the total reward over $T$ time. Some potential bidding strategies include:\n",
    "\n",
    "### 4.3 **UCB1 Algorithm:**\n",
    "\n",
    "The idea behind UCB1 is to balance exploration and exploitation by considering both the average reward of an arm and the uncertainty around that average. The arm chosen is the one with the highest upper confidence bound.\n",
    "\n",
    "For each arm $i$:\n",
    "$$\n",
    "\\text{UCB}(i) = \\bar{X}_i + \\sqrt{\\frac{2 \\ln n}{n_i}}\n",
    "$$\n",
    "Where:\n",
    "- $\\bar{X}_i$ is the average reward observed for arm $i$.\n",
    "- $n$ is the total number of pulls across all arms.\n",
    "- $n_i$ is the number of times arm $i$ has been pulled.\n",
    "\n",
    "The arm with the highest UCB is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Class for implenting the Upper Confidence Bound (UCB) algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class UCB1:\n",
    "    def __init__(self, n_arms):\n",
    "        self.counts = [0] * n_arms  # Number of times each arm has been pulled\n",
    "        self.values = [0.] * n_arms  # Average reward for each arm\n",
    "        self.n_arms = n_arms\n",
    "\n",
    "    def select_arm(self):\n",
    "        # If there's an arm we haven't pulled yet, prioritize that\n",
    "        for arm in range(self.n_arms):\n",
    "            if self.counts[arm] == 0:\n",
    "                return arm\n",
    "\n",
    "        # Calculate the UCB for each arm\n",
    "        total_counts = sum(self.counts)\n",
    "        ucb_values = [0.0 for arm in range(self.n_arms)]\n",
    "        for arm in range(self.n_arms):\n",
    "            bonus = math.sqrt((2 * math.log(total_counts)) / float(self.counts[arm]))\n",
    "            ucb_values[arm] = self.values[arm] + bonus\n",
    "\n",
    "        # Return the arm with the highest UCB\n",
    "        return ucb_values.index(max(ucb_values))\n",
    "\n",
    "    def update(self, chosen_arm, reward):\n",
    "        # Update the counts for the chosen arm\n",
    "        self.counts[chosen_arm] += 1\n",
    "        n = self.counts[chosen_arm]\n",
    "\n",
    "        # Update the average reward for the chosen arm\n",
    "        value = self.values[chosen_arm]\n",
    "        new_value = ((n - 1) / float(n)) * value + (1 / float(n)) * reward\n",
    "        self.values[chosen_arm] = new_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the above `UCB1` class:\n",
    "\n",
    "1. Initialize the bandit with the number of arms.\n",
    "2. Use the `select_arm` method to choose an arm to pull.\n",
    "3. Obtain a reward for pulling the arm (this will come from the environment or your simulation).\n",
    "4. Use the `update` method to update the bandit's knowledge based on the observed reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Bidding Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6 Potential Bidding Algorithms (Arms):\n",
    "\n",
    "1. **Fixed Bid**: Always bid a fixed amount, regardless of the player or context.\n",
    "2. **Percentage of Budget**: Bid a fixed percentage of the remaining budget.\n",
    "3. **Value-Based**: Bid based on the perceived value of the player. This might be a fixed percentage of the player's perceived value or some function of it.\n",
    "4. **Reactive Strategy**: Adjust your bid based on the bids of other teams in previous rounds. If competition seems fierce, this strategy might opt to bid more aggressively.\n",
    "5. **Random Strategy**: Bid a random amount within some predefined range. This introduces unpredictability.\n",
    "6. **Optimal Team Composition**: Use an algorithm like the integer multi-commodity network flow algorithm discussed earlier to determine the value of a player in the context of an optimal team composition, and bid accordingly.\n",
    "7. **Machine Learning Model**: Train a machine learning model on historical data to predict the optimal bid based on various features (player attributes, current team composition, etc.).\n",
    "8. **Epsilon-Greedy or Thompson Sampling**: While these are traditionally used as exploration strategies in bandit problems, they can be adapted as bidding strategies, where exploration is bidding slightly differently from the perceived optimal strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 **The Value Based Strategy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_based_bid(player_value, factor=1.0, max_bid=None):\n",
    "    \"\"\"\n",
    "    Compute the bid amount based on the perceived value of a player.\n",
    "\n",
    "    Parameters:\n",
    "    - player_value (float): The perceived value of the player.\n",
    "    - factor (float): A multiplier to determine the bid amount based on the player's value.\n",
    "    - max_bid (float or None): An optional maximum bid amount. If set, the bid will not exceed this amount.\n",
    "\n",
    "    Returns:\n",
    "    - float: The computed bid amount.\n",
    "    \"\"\"\n",
    "\n",
    "    bid_amount = player_value * factor\n",
    "    \n",
    "    if max_bid is not None:\n",
    "        bid_amount = min(bid_amount, max_bid)\n",
    "    \n",
    "    return bid_amount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example and Explanation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "player_value = 100  # Example value for a player\n",
    "factor = 0.8  # We're willing to bid up to 80% of the perceived value\n",
    "max_bid_amount = 75  # We don't want to bid more than 75 units for any player\n",
    "\n",
    "bid_amount = value_based_bid(player_value, factor, max_bid_amount)\n",
    "print(f\"Bid Amount: {bid_amount}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Determine the perceived value $v_i$ of player $i$. This could be based on expected performance metrics, historical data, or other relevant factors.\n",
    "2. Decide on a factor or percentage of this perceived value to use as the bid amount. This factor can be constant for all players, or it can be dynamic based on other factors such as remaining budget, importance of the role, or competition.\n",
    "3. Compute the bid amount for player $i$ as $\\text{bid}_i = \\text{factor} \\times v_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 **The Reactive Strategy** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReactiveBiddingStrategy:\n",
    "    def __init__(self, initial_bid_factor=1.0):\n",
    "        self.bid_history = []\n",
    "        self.initial_bid_factor = initial_bid_factor\n",
    "    \n",
    "    def record_bid(self, bid_amount):\n",
    "        \"\"\"Record a competitor's bid amount.\"\"\"\n",
    "        self.bid_history.append(bid_amount)\n",
    "    \n",
    "    def compute_bid(self, player_value):\n",
    "        \"\"\"\n",
    "        Compute the bid amount based on the perceived value of a player and the observed bids.\n",
    "\n",
    "        Parameters:\n",
    "        - player_value (float): The perceived value of the player.\n",
    "\n",
    "        Returns:\n",
    "        - float: The computed bid amount.\n",
    "        \"\"\"\n",
    "        if not self.bid_history:\n",
    "            # If no bid history is available, bid based on the initial factor\n",
    "            return player_value * self.initial_bid_factor\n",
    "        \n",
    "        # Compute average of past bids\n",
    "        avg_bid = sum(self.bid_history) / len(self.bid_history)\n",
    "        \n",
    "        # Adjust bid based on observed average. \n",
    "        # Here, we're simply bidding slightly more than the average observed bid.\n",
    "        # This can be adjusted or made more sophisticated based on specific strategy or observed patterns.\n",
    "        bid_factor = avg_bid / player_value + 0.05  # Bidding 5% more than the average observed bid factor\n",
    "        \n",
    "        return player_value * bid_factor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example and Explanation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "strategy = ReactiveBiddingStrategy(initial_bid_factor=0.8)\n",
    "strategy.record_bid(80)  # Say a competitor bid 80 in the past for a player of value 100\n",
    "strategy.record_bid(85)  # Another bid from a competitor for a similar valued player\n",
    "\n",
    "player_value = 100\n",
    "bid_amount = strategy.compute_bid(player_value)\n",
    "print(f\"Bid Amount: {bid_amount}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Reactive Strategy involves adjusting bids based on observed bids from competitors in previous rounds. The idea is to recognize patterns or behaviors of competitors and adjust bids accordingly to increase the chances of acquiring desired players.\n",
    "\n",
    "To implement this, we need to maintain a history of past bids from competitors. Using this history, we can compute average bids, recognize aggressive or conservative bidders, or detect other patterns.\n",
    "\n",
    "This example uses the average of observed bids to compute the bid factor, then bids slightly more than the average to outbid competitors. This is a basic reactive strategy, and it can be made more sophisticated by considering other factors, such as:\n",
    "- Recognizing aggressive or conservative bidders and adjusting strategy accordingly.\n",
    "- Considering other statistics like median, mode, or standard deviation of past bids.\n",
    "- Incorporating other contextual information like remaining budget, importance of the player, etc.\n",
    "\n",
    "The key with a reactive strategy is to adjust bids based on observed competitor behavior to maximize the chances of success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 **The $\\epsilon$-greedy strategy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class EpsilonGreedyBiddingStrategy:\n",
    "    def __init__(self, epsilon=0.1, exploitation_factor=0.8):\n",
    "        self.epsilon = epsilon\n",
    "        self.exploitation_factor = exploitation_factor\n",
    "    \n",
    "    def compute_bid(self, player_value, min_bid=0, max_bid=None):\n",
    "        \"\"\"\n",
    "        Compute the bid amount based on the epsilon-greedy strategy.\n",
    "\n",
    "        Parameters:\n",
    "        - player_value (float): The perceived value of the player.\n",
    "        - min_bid (float): Minimum bid amount. Used for exploration.\n",
    "        - max_bid (float or None): Maximum bid amount. Used for exploration and to cap exploitation bid.\n",
    "\n",
    "        Returns:\n",
    "        - float: The computed bid amount.\n",
    "        \"\"\"\n",
    "\n",
    "        # Exploration: Bid randomly\n",
    "        if random.random() < self.epsilon:\n",
    "            random_bid = random.uniform(min_bid, player_value if max_bid is None else max_bid)\n",
    "            return random_bid\n",
    "        \n",
    "        # Exploitation: Bid based on perceived value\n",
    "        bid_amount = player_value * self.exploitation_factor\n",
    "        \n",
    "        if max_bid is not None:\n",
    "            bid_amount = min(bid_amount, max_bid)\n",
    "        \n",
    "        return bid_amount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example and Explanation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "strategy = EpsilonGreedyBiddingStrategy(epsilon=0.1, exploitation_factor=0.85)\n",
    "\n",
    "player_value = 100\n",
    "bid_amount = strategy.compute_bid(player_value, min_bid=50, max_bid=90)\n",
    "print(f\"Bid Amount: {bid_amount}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\epsilon$-greedy strategy is a simple yet effective method primarily used in the exploration-exploitation dilemma, such as in reinforcement learning. The idea is to take the best known action most of the time (exploitation) but occasionally (with probability $\\epsilon$) take a random action (exploration).\n",
    "\n",
    "In this basic implementation of the \\(\\epsilon\\)-greedy bidding strategy we do the following:\n",
    "\n",
    "1. With probability $\\epsilon$, bid randomly (exploration).\n",
    "2. With probability $1 - \\epsilon$, bid according to the perceived value of the player or based on historical data (exploitation).\n",
    "\n",
    "\n",
    "In this example, the strategy will bid 85% of the player's perceived value 90% of the time (exploitation) and will bid a random amount between 50 and 90 10% of the time (exploration). Adjusting the `epsilon` parameter allows you to control the balance between exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 **The Optimal Composition Strategy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses the optimal team composition described above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Warm starting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A warm start that runs simulations using synthetic data (informed by historical data) to initialize the model prior to its actual usage. For this setup, we'll:\n",
    "\n",
    "1. Create individual bidding strategies: epsilon-greedy, reactive, value-based, and optimal team composition using linear programming.\n",
    "2. Integrate these strategies as arms in the UCB1 multi-armed bandit framework.\n",
    "3. Use synthetic data (based on historical data) to simulate the bidding rounds and update the UCB1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from abc import ABC, abstractmethod\n",
    "import pulp\n",
    "\n",
    "role_requirements = {\n",
    "    'forward': 3,\n",
    "    'midfielder': 4,\n",
    "    'defender': 3,\n",
    "    'goalkeeper': 1\n",
    "}\n",
    "\n",
    "class BaseBiddingStrategy:\n",
    "    def __init__(self, players):\n",
    "        self.players = players\n",
    "        self.acquired_players = []\n",
    "\n",
    "    def can_acquire(self, player_index):\n",
    "        player_role = self.players[player_index]['role']\n",
    "        current_role_count = sum(1 for p in self.acquired_players if self.players[p]['role'] == player_role)\n",
    "        return current_role_count < role_requirements[player_role]\n",
    "\n",
    "class EpsilonGreedy(BaseBiddingStrategy):\n",
    "    def __init__(self, players, epsilon=0.1, exploitation_factor=0.8):\n",
    "        super().__init__(players)\n",
    "        self.epsilon = epsilon\n",
    "        self.exploitation_factor = exploitation_factor\n",
    "\n",
    "    def compute_bid(self, player_index):\n",
    "        if not self.can_acquire(player_index):\n",
    "            return 0\n",
    "        player_value = self.players[player_index]['value']\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.uniform(0, player_value)\n",
    "        return player_value * self.exploitation_factor\n",
    "\n",
    "class Reactive(BaseBiddingStrategy):\n",
    "    def __init__(self, players, initial_bid_factor=1.0):\n",
    "        super().__init__(players)\n",
    "        self.bid_history = []\n",
    "        self.initial_bid_factor = initial_bid_factor\n",
    "\n",
    "    def compute_bid(self, player_index):\n",
    "        if not self.can_acquire(player_index):\n",
    "            return 0\n",
    "        player_value = self.players[player_index]['value']\n",
    "        if not self.bid_history:\n",
    "            return player_value * self.initial_bid_factor\n",
    "        avg_bid = sum(self.bid_history) / len(self.bid_history)\n",
    "        bid_factor = avg_bid / player_value + 0.05\n",
    "        return player_value * bid_factor\n",
    "\n",
    "class ValueBased(BaseBiddingStrategy):\n",
    "    def compute_bid(self, player_index):\n",
    "        if not self.can_acquire(player_index):\n",
    "            return 0\n",
    "        player_value = self.players[player_index]['value']\n",
    "        return player_value * 0.8\n",
    "\n",
    "class OptimalTeamCompositionLP(BaseBiddingStrategy):\n",
    "    def compute_bid(self, player_index):\n",
    "        if not self.can_acquire(player_index):\n",
    "            return 0\n",
    "        player_value = self.players[player_index]['value']\n",
    "        return 0.9 * player_value\n",
    "\n",
    "\n",
    "# UCB1 with different bidding strategies as arms:\n",
    "class UCB1:\n",
    "    def __init__(self, strategies):\n",
    "        self.strategies = strategies\n",
    "        self.counts = [0] * len(strategies)\n",
    "        self.values = [0] * len(strategies)\n",
    "\n",
    "    def select_arm(self):\n",
    "        n_arms = len(self.strategies)\n",
    "        \n",
    "        for i in range(n_arms):\n",
    "            if self.counts[i] == 0:\n",
    "                return i\n",
    "\n",
    "        total_counts = sum(self.counts)\n",
    "        ucb_values = [\n",
    "            self.values[i] + math.sqrt((2 * math.log(total_counts)) / self.counts[i])\n",
    "            for i in range(n_arms)\n",
    "        ]\n",
    "        return ucb_values.index(max(ucb_values))\n",
    "\n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] += 1\n",
    "        n = self.counts[chosen_arm]\n",
    "        value = self.values[chosen_arm]\n",
    "        self.values[chosen_arm] = ((n - 1) / n) * value + (1 / n) * reward\n",
    "\n",
    "\n",
    "def generate_historical_data(n=100):\n",
    "    roles = ['forward', 'midfielder', 'defender', 'goalkeeper']\n",
    "    players = []\n",
    "\n",
    "    for _ in range(n):\n",
    "        value = random.randint(80, 150)\n",
    "        cost = int(value * random.uniform(0.6, 0.9))\n",
    "        role = random.choice(roles)\n",
    "        \n",
    "        player = {\n",
    "            'value': value,\n",
    "            'cost': cost,\n",
    "            'role': role\n",
    "        }\n",
    "        players.append(player)\n",
    "\n",
    "    return players\n",
    "\n",
    "\n",
    "# Simulation using synthetic data based on historical data:\n",
    "def warm_start_simulation(ucb1_model, historical_data, n_rounds=None):\n",
    "    if n_rounds is None:\n",
    "        n_rounds = len(historical_data)\n",
    "\n",
    "    for i in range(n_rounds):\n",
    "        player_index = i  # Assuming players are bid on in order\n",
    "\n",
    "        # Simulate a competitive bid from other bidders\n",
    "        player_value = historical_data[player_index]['value']\n",
    "        competitive_bid = player_value * random.uniform(0.7, 1.2)  # Between 70% to 120% of player value\n",
    "\n",
    "        chosen_strategy_idx = ucb1_model.select_arm()\n",
    "        chosen_strategy = ucb1_model.strategies[chosen_strategy_idx]\n",
    "        \n",
    "        bid = chosen_strategy.compute_bid(player_index)\n",
    "        \n",
    "        # Simulating the reward: winning the player if our bid exceeds the competitive bid.\n",
    "        reward = 1 if bid > competitive_bid else 0\n",
    "        ucb1_model.update(chosen_strategy_idx, reward)\n",
    "\n",
    "        if reward == 1:\n",
    "            chosen_strategy.acquired_players.append(player_index)\n",
    "            chosen_strategy.remaining_budget -= historical_data[player_index]['cost']\n",
    "\n",
    "\n",
    "# Generate historical data for 100 players\n",
    "historical_data = generate_historical_data()\n",
    "print(historical_data[:10])  # Displaying the first 10 players as an example\n",
    "\n",
    "# Initial Budget\n",
    "initial_budget = 1000\n",
    "\n",
    "# Initialize strategies with the historical_data and initial_budget\n",
    "strategies = [\n",
    "    EpsilonGreedy(historical_data, initial_budget),\n",
    "    Reactive(historical_data, initial_budget),\n",
    "    ValueBased(historical_data, initial_budget),\n",
    "    OptimalTeamCompositionLP(historical_data, initial_budget)\n",
    "]\n",
    "\n",
    "# Printing the strategies to verify initialization\n",
    "for strategy in strategies:\n",
    "    print(type(strategy).__name__, \"-> Remaining Budget:\", strategy.remaining_budget)\n",
    "\n",
    "\n",
    "# Initialize UCB1 model with the strategies\n",
    "ucb1_model = UCB1(strategies)\n",
    "warm_start_simulation(ucb1_model, historical_data, n_rounds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overkill..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorpoate a Neural Network arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Certainly! Creating a neural network strategy involves multiple steps:\n",
    "\n",
    "1. **Data Preparation**: Historic and synthetic data should be structured to create features (input) and targets (output bids).\n",
    "2. **Model Architecture**: Define a neural network architecture suitable for the task.\n",
    "3. **Training**: Use the prepared data to train the neural network.\n",
    "4. **Bidding Strategy**: Integrate the neural network into the `BaseBiddingStrategy`.\n",
    "\n",
    "Let's start!\n",
    "\n",
    "### 1. Data Preparation\n",
    "\n",
    "Suppose our historic and synthetic data has the following structure for each player:\n",
    "\n",
    "- `'value'`: Value of the player.\n",
    "- `'cost'`: Cost of the player.\n",
    "- `'role'`: Role of the player (encoded as integers, e.g., 0 for forward, 1 for midfielder, etc.)\n",
    "- `'other_bids'`: List of bids made by other groups for this player.\n",
    "- `'optimum_team_composition'`: List of player indices representing the optimal team composition.\n",
    "- `'budget'`: Available budget before bidding for this player.\n",
    "- `'bid'`: Our group's bid for the player (target for training).\n",
    "\n",
    "### 2. Model Architecture\n",
    "\n",
    "We'll design a simple feedforward neural network for this task. The model will take in the features and output a single value representing the bid.\n",
    "\n",
    "### 3. Training\n",
    "\n",
    "We'll use the PyTorch library to define, train, and evaluate the model.\n",
    "\n",
    "### 4. Bidding Strategy\n",
    "\n",
    "We'll integrate the trained model into the `BaseBiddingStrategy`.\n",
    "\n",
    "Note: \n",
    "\n",
    "- The data structure and feature extraction in this code is for illustrative purposes. Depending on the actual data you have, you might need to adjust the data extraction and preprocessing steps.\n",
    "- The neural network architecture is basic; for a real-world task, it may require fine-tuning or a more sophisticated design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the neural network architecture\n",
    "class BiddingNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BiddingNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Training the model\n",
    "def train_model(model, data, epochs=100, lr=0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for player in data:\n",
    "            # Extract features and target\n",
    "            features = torch.tensor([player['value'], player['cost'], player['role'], \n",
    "                                     *player['other_bids'], *player['optimum_team_composition'], \n",
    "                                     player['budget']])\n",
    "            target = torch.tensor([player['bid']])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "class NeuralNetworkStrategy(BaseBiddingStrategy):\n",
    "    def __init__(self, players, initial_budget, model):\n",
    "        super().__init__(players, initial_budget)\n",
    "        self.model = model\n",
    "\n",
    "    def compute_bid(self, player_index):\n",
    "        player = self.players[player_index]\n",
    "        features = torch.tensor([player['value'], player['cost'], player['role'], \n",
    "                                 *player['other_bids'], *player['optimum_team_composition'], \n",
    "                                 self.remaining_budget])\n",
    "        with torch.no_grad():\n",
    "            bid = self.model(features).item()\n",
    "        return bid\n",
    "\n",
    "# Sample code to train and use the strategy\n",
    "# model = BiddingNN(input_size=...)  # Define appropriate input size\n",
    "# train_model(model, historical_data)\n",
    "# nn_strategy = NeuralNetworkStrategy(historical_data, initial_budget, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Multi-Arm Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCB1 algorithm is primarily an exploration-exploitation strategy for multi-armed bandit problems. We can create an online-machine learning hybrid approach where we use a model (potentially a neural network) to predict the expected rewards of the arms based on some context, and we optimize this model using Adam. This fits more into the realm of contextual bandits, where the context can be used to decide which arm to pull.\n",
    "\n",
    "Here's a high-level approach:\n",
    "\n",
    "1. **Neural Network Model**: Define a neural network where the input is the context and the output is the predicted reward for each arm.\n",
    "2. **UCB1 Strategy**: Use the UCB1 algorithm to decide which arm to pull based on the predicted rewards and the uncertainty about these predictions. The uncertainty can be captured using the number of times each arm has been pulled (similar to traditional UCB1) or other mechanisms.\n",
    "3. **Update Using Adam**: After pulling an arm and observing a reward, update the neural network model using the observed reward as the ground truth. The loss can be the difference between the predicted reward and the observed reward for the chosen arm. Use the Adam optimizer to perform this update.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ContextualBandit(nn.Module):\n",
    "    def __init__(self, input_dim, n_arms):\n",
    "        super(ContextualBandit, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, n_arms)  # Simple linear model\n",
    "        self.n_arms = n_arms\n",
    "        self.counts = torch.zeros(n_arms)\n",
    "        self.values = torch.zeros(n_arms)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "    def select_arm(self, x):\n",
    "        predicted_rewards = self.forward(x)\n",
    "        upper_bound = torch.sqrt(2 * torch.log(sum(self.counts)) / (self.counts + 1e-5))\n",
    "        ucb_values = predicted_rewards + upper_bound\n",
    "        arm = torch.argmax(ucb_values).item()\n",
    "        return arm\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        value = self.values[arm]\n",
    "        n = self.counts[arm]\n",
    "        self.values[arm] = ((n - 1) / n) * value + (1 / n) * reward\n",
    "\n",
    "# Example usage:\n",
    "input_dim = 5  # Context size\n",
    "n_arms = 3  # Number of arms/strategies\n",
    "\n",
    "bandit = ContextualBandit(input_dim, n_arms)\n",
    "optimizer = optim.Adam(bandit.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    # Generate a synthetic context\n",
    "    context = torch.randn(input_dim)\n",
    "    \n",
    "    # Use UCB1 to select an arm\n",
    "    chosen_arm = bandit.select_arm(context)\n",
    "    \n",
    "    # Get a reward from the environment (you'd replace this with your actual reward mechanism)\n",
    "    reward = torch.randn()  # Random reward as an example\n",
    "    \n",
    "    # Compute the loss\n",
    "    predicted_rewards = bandit(context)\n",
    "    loss = (predicted_rewards[chosen_arm] - reward)**2\n",
    "    \n",
    "    # Optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update UCB1 values\n",
    "    bandit.update(chosen_arm, reward.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example provides a basic integration of UCB1 with a PyTorch model optimized using Adam. Note that this is a simple linear model, but you can replace it with more complex architectures depending on the problem. This approach allows you to leverage the power of neural networks to predict rewards based on context, while still benefiting from the exploration-exploitation trade-off provided by UCB1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Circumventing complexity and ensuring you are never without a bid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Outsourcing to Dedicated Machines in Parallel:\n",
    "\n",
    "**Pros**:\n",
    "- **Speed**: Running computations in parallel on dedicated machines can drastically reduce the time taken to get results from computationally expensive arms.\n",
    "- **Scalability**: This approach scales well. As the number of complex arms increases, you can increase the number of machines accordingly.\n",
    "\n",
    "**Cons**:\n",
    "- **Cost**: Running multiple dedicated machines can be expensive.\n",
    "- **Complexity**: Managing distributed computation and ensuring synchronization can be complex.\n",
    "\n",
    "**Implementation**:\n",
    "- Use distributed computing frameworks like **Celery** for task distribution and results aggregation.\n",
    "- Cloud platforms like **AWS Lambda** or **Google Cloud Functions** can also serve as scalable options for parallel computation.\n",
    "\n",
    "### 2. Pre-compute Likely Scenarios:\n",
    "\n",
    "**Pros**:\n",
    "- **Predictive Power**: By anticipating future scenarios, you can pre-compute and cache results, making the actual decision phase much faster.\n",
    "- **Resource Utilization**: During downtime or idle periods, computational resources can be used to pre-compute these scenarios.\n",
    "\n",
    "**Cons**:\n",
    "- **Accuracy**: Predicting future scenarios can be tricky, and there's always a chance that the actual scenario doesn't match any of the pre-computed ones.\n",
    "\n",
    "**Implementation**:\n",
    "- **State Estimation**: Use the current state (available players, budget, etc.) to estimate potential future states.\n",
    "- **Caching**: Store results for these estimated states in a cache (like **Redis**). When a similar state occurs, retrieve the result from the cache.\n",
    "\n",
    "### 3. Best-case Fallback Votes:\n",
    "\n",
    "**Pros**:\n",
    "- **Safety Net**: Provides a default action in case time runs out or there are unforeseen computational delays.\n",
    "- **Consistency**: Even if it's not the optimal choice, having a reasonable fallback ensures you always have a valid bid.\n",
    "\n",
    "**Cons**:\n",
    "- **Sub-optimality**: The fallback action might not be the best decision in certain contexts.\n",
    "\n",
    "**Implementation**:\n",
    "- **Heuristics**: Develop heuristics or rules that define a good-enough bid based on historical data or domain knowledge.\n",
    "- **Timers**: Use timers to track decision-making time. If a threshold is reached without a decision from the main algorithm, trigger the fallback.\n",
    "\n",
    "### Additional Considerations:\n",
    "\n",
    "- **Model Simplification**: For computationally expensive arms, consider if there are simpler models or approximations that can provide reasonably good results in less time.\n",
    "- **Adaptive Exploration**: If certain arms consistently prove to be computationally expensive without offering significantly better rewards, the algorithm could reduce the frequency with which they're selected.\n",
    "- **Asynchronous Updates**: If updating the model is part of the computational expense, consider updating the model asynchronously, decoupling the decision-making process from the model update process.\n",
    "\n",
    "In summary, the combination of outsourcing to dedicated machines, pre-computing likely scenarios, and having fallback votes provides a comprehensive strategy to handle the computational challenges of complex bandit arms. Properly implemented, these strategies can ensure timely and effective decisions in a dynamic bidding environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
